# agents/summarizer_agent.py
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
from core.logger import get_logger

load_dotenv()
logger = get_logger(__name__, "summarizer.log")


SUMMARIZER_PROMPT = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ. –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ—Ç—á—ë—Ç.
–¢—ã –ù–ï –º–æ–∂–µ—à—å –≤—ã–¥—É–º—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –Ω–∞–ø–∏—à–∏ "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö".

–¶–µ–ª—å: –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: ¬´–ö–∞–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç –≥—Ä—É–ø–ø—É 1 –æ—Ç –≥—Ä—É–ø–ø—ã 0?¬ª

–§–æ—Ä–º–∞—Ç –æ—Ç—á—ë—Ç–∞:
# –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç –¥–ª—è —Ñ–∞–π–ª–∞ {filename}

## –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã
{insights_list}

## –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
### 1. –ü–æ–∏—Å–∫ –≥–ª–∞–≤–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
{primary_feature_summary}

### 2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
{correlation_summary}

### 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
{descriptive_stats_summary}

### 4. –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
{categorical_analysis_summary}

### 5. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏
{full_model_summary}

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ–±—â–∏–π –≤—ã–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤.
–û–±—ä—è—Å–Ω–∏, –∫–∞–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç –≥—Ä—É–ø–ø—É 1 –æ—Ç –≥—Ä—É–ø–ø—ã 0.
–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî –Ω–∞–ø–∏—à–∏ "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö".
"""


def generate_summary(insights: list, tool_results: list, filename: str = "unknown.csv") -> str:
    logger.info(f"üìù Summarizer Agent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")

    llm = ChatOpenAI(
        model="qwen2.5-32b-instruct",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
        temperature=0.3
    )

    results_map = {res["tool_name"]: res for res in tool_results}

    def get_summary(name: str) -> str:
        res = results_map.get(name)
        return res["summary"] if res and res["status"] == "success" else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    insights_list = "\n".join([f"- {s}" for s in insights]) if insights else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    prompt = ChatPromptTemplate.from_template(SUMMARIZER_PROMPT)
    chain = prompt | llm

    response = chain.invoke({
        "filename": filename,
        "insights_list": insights_list,
        "primary_feature_summary": get_summary("PrimaryFeatureFinder"),
        "correlation_summary": get_summary("CorrelationAnalysis"),
        "descriptive_stats_summary": get_summary("DescriptiveStatsComparator"),
        "categorical_analysis_summary": get_summary("CategoricalFeatureAnalysis"),
        "full_model_summary": get_summary("FullModelFeatureImportance"),
    })

    return response.content