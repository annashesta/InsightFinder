# ui/gradio_app.py
"""
Gradio UI –¥–ª—è InsightFinder.
"""

import os
import io
import zipfile
import re
import base64
import tempfile
from pathlib import Path
from typing import Tuple, List, Optional

import gradio as gr
import pandas as pd

from core.pipeline import analyze_dataset
from core.logger import get_logger
from core.utils import find_binary_target

# --- –ò–º–ø–æ—Ä—Ç OpenAI ---
try:
    from openai import OpenAI

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print(
        "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'openai' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. "
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏: `pip install openai`"
    )

logger = get_logger(__name__, "gradio_app.log")


# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ---
class MarkdownImageProcessor:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç Markdown, –Ω–∞—Ö–æ–¥—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤—Å—Ç—Ä–∞–∏–≤–∞—è –∏—Ö –∫–∞–∫ base64."""

    def __init__(self, base_images_dir: str = "report/output/images"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä.

        Args:
            base_images_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """
        self.base_images_dir = Path(base_images_dir).resolve()
        logger.debug(
            f"MarkdownImageProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å base_images_dir: "
            f"{self.base_images_dir}"
        )

    def _image_to_base64(self, image_path: Path) -> Optional[str]:
        """
        –ß–∏—Ç–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏ base64.

        Args:
            image_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

        Returns:
            –°—Ç—Ä–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64 –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        try:
            if not image_path.exists():
                logger.warning(f"–§–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω: {image_path}")
                return None

            suffix = image_path.suffix.lower()
            if suffix == ".png":
                mime_type = "image/png"
            elif suffix in [".jpg", ".jpeg"]:
                mime_type = "image/jpeg"
            elif suffix == ".gif":
                mime_type = "image/gif"
            else:
                logger.warning(
                    f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {suffix} "
                    f"–¥–ª—è —Ñ–∞–π–ª–∞ {image_path}"
                )
                return None

            with open(image_path, "rb") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode("utf-8")
            return f"data:{mime_type};base64,{encoded_string}"
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path} –≤ base64: {e}"
            )
            return None

    def process_markdown(self, markdown_content: str) -> Tuple[str, List]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç Markdown, –Ω–∞—Ö–æ–¥—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ–¥–∏—Ä—É—è –∏—Ö –≤ base64 –∏
        –∑–∞–º–µ–Ω—è—è —Å—Å—ã–ª–∫–∏ –Ω–∞ HTML-—Ç–µ–≥–∏ <img>.

        Args:
            markdown_content: –ò—Å—Ö–æ–¥–Ω—ã–π Markdown —Ç–µ–∫—Å—Ç.

        Returns:
            –ö–æ—Ä—Ç–µ–∂ –∏–∑ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π HTML/Markdown —Ç–µ–∫—Å—Ç, –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫).
            –í—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
        """
        images_found_for_gallery = []

        def replace_image_tag(match):
            alt_text = match.group(1).strip()
            img_path_str = match.group(2).strip()
            logger.debug(
                f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: alt='{alt_text}', "
                f"path='{img_path_str}'"
            )

            clean_path_str = img_path_str
            if clean_path_str.startswith("images/"):
                clean_path_str = clean_path_str[len("images/"):]
            elif clean_path_str.startswith("report/output/images/"):
                clean_path_str = clean_path_str[len("report/output/images/"):]
            elif "report/output/images/" in clean_path_str:
                parts = clean_path_str.split("report/output/images/")
                if len(parts) > 1:
                    clean_path_str = parts[1]

            img_full_path = self.base_images_dir / clean_path_str
            data_url = self._image_to_base64(img_full_path)

            if data_url:
                logger.debug(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–æ: {img_full_path}")
                return (
                    f'<p style="text-align: center;">'
                    f'<img src="{data_url}" alt="{alt_text}" '
                    f'style="max-width: 100%; height: auto;" />'
                    f'</p>'
                    f'<p style="text-align: center; font-size: 0.9em;">'
                    f'<em>{alt_text}</em></p>'
                )
            else:
                logger.warning(
                    f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –≤—Å—Ç—Ä–æ–µ–Ω–æ, –æ—Å—Ç–∞–≤–ª–µ–Ω placeholder: {img_path_str}"
                )
                return (
                    f'<p style="text-align: center; font-style: italic;">'
                    f'[–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {alt_text}]</p>'
                )

        pattern = r"!\[(.*?)\]\(([^)]+)\)"
        processed_content = re.sub(
            pattern, replace_image_tag, markdown_content, flags=re.DOTALL
        )
        return processed_content, images_found_for_gallery


# --- –§—É–Ω–∫—Ü–∏–∏ LLM ---
def call_llm_for_qa(
        report_text: str, question: str, api_key: str, base_url: str, model: str
) -> str:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç LLM –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –æ—Ç—á–µ—Ç—É.

    Args:
        report_text: –¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞.
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        api_key: API –∫–ª—é—á.
        base_url: –ë–∞–∑–æ–≤—ã–π URL.
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç LLM.
    """
    if not OPENAI_AVAILABLE:
        return "‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'openai' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞."

    if not api_key or not base_url or not model:
        return "‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã API (–∫–ª—é—á, URL, –º–æ–¥–µ–ª—å)."

    try:
        client = OpenAI(api_key=api_key, base_url=base_url.rstrip("/") + "/v1")
        prompt = f"""
–û—Ç—á–µ—Ç:
{report_text}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ \
–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤—ã—à–µ –æ—Ç—á–µ—Ç–∞. 
–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —Ç–æ—á–Ω—ã–º. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –æ—Ç—á–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
"""
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=1024,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {str(e)}"


def call_llm_to_determine_target(
        question: str, columns: List[str], api_key: str, base_url: str, model: str
) -> str:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç LLM –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞.

    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞.
        api_key: API –∫–ª—é—á.
        base_url: –ë–∞–∑–æ–≤—ã–π URL.
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

    Returns:
        –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ç–∞—Ä–≥–µ—Ç.
    """
    if not OPENAI_AVAILABLE:
        return columns[0] if columns else ""

    if not api_key or not base_url or not model:
        return columns[0] if columns else ""

    try:
        client = OpenAI(api_key=api_key, base_url=base_url.rstrip("/") + "/v1")
        columns_str = "\n".join([f"- {col}" for col in columns])
        prompt = f"""
–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
–¢–µ–±–µ –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∞—è –∫–æ–ª–æ–Ω–∫–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π \
(—Ç–∞—Ä–≥–µ—Ç–æ–º) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤–æ–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:
{columns_str}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä–∞—è, –ø–æ —Ç–≤–æ–µ–º—É –º–Ω–µ–Ω–∏—é, 
—è–≤–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å. 
–ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω, –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é.

–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏:
"""
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=100,
        )
        target = response.choices[0].message.content.strip()
        if target in columns:
            return target
        else:
            return columns[0] if columns else ""
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞: {e}")
        return columns[0] if columns else ""


# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ Gradio ---
def run_analysis(
        file_obj,
        api_key: str,
        base_url: str,
        model: str,
        question_for_target: str,
) -> Tuple[str, str, List, str, str]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞.

    Args:
        file_obj: –û–±—ä–µ–∫—Ç —Ñ–∞–π–ª–∞ Gradio.
        api_key: API –∫–ª—é—á.
        base_url: –ë–∞–∑–æ–≤—ã–π URL.
        model: –ú–æ–¥–µ–ª—å.
        question_for_target: –í–æ–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞.

    Returns:
        –ö–æ—Ä—Ç–µ–∂: (—Å—Ç–∞—Ç—É—Å, –ø—É—Ç—å_–∫_–æ—Ç—á–µ—Ç—É, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–µ–∫—Å—Ç_–æ—Ç—á–µ—Ç–∞, –∏—Å—Ç–æ—Ä–∏—è)
    """
    if not file_obj:
        return "‚ùå –§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.", "", [], "", ""

    if not question_for_target:
        return (
            "‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞.",
            "", [], "", ""
        )

    try:
        df = pd.read_csv(file_obj.name)
        logger.info("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª —á–µ—Ä–µ–∑ Gradio UI.")

        binary_cols = [col for col in df.columns if df[col].nunique() == 2]
        if binary_cols:
            determined_target = call_llm_to_determine_target(
                question_for_target, binary_cols, api_key, base_url, model
            )
            if determined_target in binary_cols:
                target_col = determined_target
                logger.info(f"üéØ –¢–∞—Ä–≥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω LLM: '{target_col}'")
            else:
                target_col = binary_cols[0]
                logger.info(
                    f"üéØ –¢–∞—Ä–≥–µ—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ø–µ—Ä–≤–∞—è –±–∏–Ω–∞—Ä–Ω–∞—è): '{target_col}'"
                )
        else:
            target_col = find_binary_target(df)
            logger.info(f"üéØ –ù–∞–π–¥–µ–Ω –±–∏–Ω–∞—Ä–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: '{target_col}'")

        unique_vals = df[target_col].dropna().unique()
        if len(unique_vals) != 2:
            return (
                f"‚ùå –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü '{target_col}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –±–∏–Ω–∞—Ä–Ω—ã–º. "
                f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {unique_vals}.",
                "", [], "", ""
            )

        with tempfile.NamedTemporaryFile(
                mode="w", delete=False, suffix=".csv", encoding="utf-8"
        ) as tmpfile:
            df.to_csv(tmpfile.name, index=False)
            tmp_path = tmpfile.name

        report_path, history, report_text = analyze_dataset(tmp_path, target_col)
        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω.")

        os.unlink(tmp_path)

        processor = MarkdownImageProcessor()
        processed_markdown, images = processor.process_markdown(report_text)

        return (
            "‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!",
            report_path,
            images,
            report_text,
            str(history),
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ run_analysis: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}", "", [], "", ""


def answer_question(
        question: str, report_text: str, api_key: str, base_url: str, model: str
) -> str:
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –æ—Ç—á–µ—Ç—É."""
    if not question:
        return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å."
    if not report_text:
        return "–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç—á–µ—Ç."

    logger.info(f"‚ùì –í–æ–ø—Ä–æ—Å –ø–æ –æ—Ç—á–µ—Ç—É: {question}")
    answer = call_llm_for_qa(report_text, question, api_key, base_url, model)
    logger.info("‚úÖ –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—É—á–µ–Ω.")
    return answer


def create_zip_with_images(report_text: str) -> Optional[str]:
    """–°–æ–∑–¥–∞–µ—Ç ZIP –∞—Ä—Ö–∏–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–∑ –æ—Ç—á–µ—Ç–∞."""
    if not report_text:
        return None

    try:
        pattern = r"!\[.*?\]\((.*?)\)"
        image_paths = re.findall(pattern, report_text)

        if not image_paths:
            return None

        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zip_file:
            base_images_dir = Path("report/output/images")
            added_files = set()

            for img_path_str in image_paths:
                clean_path_str = img_path_str
                if clean_path_str.startswith("images/"):
                    clean_path_str = clean_path_str[len("images/"):]
                elif clean_path_str.startswith("report/output/images/"):
                    clean_path_str = clean_path_str[len("report/output/images/"):]
                elif "report/output/images/" in clean_path_str:
                    parts = clean_path_str.split("report/output/images/")
                    if len(parts) > 1:
                        clean_path_str = parts[1]

                img_full_path = base_images_dir / clean_path_str
                if img_full_path.exists() and img_full_path not in added_files:
                    arcname = Path(clean_path_str).name
                    zip_file.write(img_full_path, arcname)
                    added_files.add(img_full_path)

        zip_buffer.seek(0)

        with tempfile.NamedTemporaryFile(
                delete=False, suffix=".zip"
        ) as tmp_zip:
            tmp_zip.write(zip_buffer.getvalue())
            return tmp_zip.name
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è ZIP —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {e}")
        return None


def fetch_models(api_key: str, base_url: str):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç API."""
    if not api_key or not base_url:
        return gr.update(choices=[], value="", interactive=True)
    try:
        client = OpenAI(api_key=api_key, base_url=base_url.rstrip("/") + "/v1")
        models_response = client.models.list()
        model_ids = sorted([model.id for model in models_response.data])
        return gr.update(
            choices=model_ids,
            value=model_ids[0] if model_ids else "",
            interactive=True,
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return gr.update(choices=[], value="", interactive=True)


def save_api_settings(api_key: str, base_url: str, model: str) -> str:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API –≤ .env —Ñ–∞–π–ª."""
    try:
        with open(".env", "w", encoding="utf-8") as f:
            f.write(f"OPENAI_API_KEY={api_key}\n")
            f.write(f"OPENAI_BASE_URL={base_url}\n")
            f.write(f"OPENAI_MODEL={model}\n")

        os.environ["OPENAI_API_KEY"] = api_key
        os.environ["OPENAI_BASE_URL"] = base_url
        os.environ["OPENAI_MODEL"] = model

        logger.info("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ .env")
        return "‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ API: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {str(e)}"


# --- Gradio –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
def build_interface():
    """–°–æ–∑–¥–∞–µ—Ç Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å."""
    with gr.Blocks(title="InsightFinder") as demo:
        gr.Markdown("# InsightFinder ‚Äî AI –∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö")
        # gr.Image("insightFinderLogo.png", elem_id="logo", show_label=False,
        # container=False, height=100)

        report_text_state = gr.State("")
        history_state = gr.State("")

        with gr.Tab("–ê–Ω–∞–ª–∏–∑"):
            with gr.Row():
                with gr.Column(scale=1):
                    file_input = gr.File(
                        label="üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª", file_types=[".csv"]
                    )

                    with gr.Accordion("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API", open=True):
                        api_key_input = gr.Textbox(
                            label="OPENAI_API_KEY",
                            type="password",
                            value=os.getenv("OPENAI_API_KEY", ""),
                        )
                        base_url_input = gr.Textbox(
                            label="OPENAI_BASE_URL",
                            value=os.getenv(
                                "OPENAI_BASE_URL",
                                "https://openai-hub.neuraldeep.tech  "
                            ).strip(),
                        )

                        fetch_models_btn = gr.Button("üîÑ –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")

                        if OPENAI_AVAILABLE:
                            initial_models = []
                            initial_model = os.getenv(
                                "OPENAI_MODEL", "qwen2.5-32b-instruct"
                            )
                            try:
                                client = OpenAI(
                                    api_key=os.getenv("OPENAI_API_KEY", ""),
                                    base_url=os.getenv(
                                        "OPENAI_BASE_URL",
                                        "https://openai-hub.neuraldeep.tech  "
                                    ).rstrip("/") + "/v1",
                                )
                                models_response = client.models.list()
                                initial_models = sorted(
                                    [model.id for model in models_response.data]
                                )
                                if (initial_model not in initial_models and
                                        initial_models):
                                    initial_model = initial_models[0]
                            except Exception as e:
                                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏: {e}")

                            model_dropdown = gr.Dropdown(
                                choices=initial_models,
                                value=initial_model,
                                label="OPENAI_MODEL",
                                interactive=True,
                            )
                        else:
                            model_input = gr.Textbox(
                                label="OPENAI_MODEL",
                                value=os.getenv(
                                    "OPENAI_MODEL", "qwen2.5-32b-instruct"
                                ),
                            )

                        save_settings_btn = gr.Button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                        settings_status = gr.Textbox(
                            label="", interactive=False, visible=False
                        )

                    question_for_target_input = gr.Textbox(
                        label="‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ "
                              "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞",
                        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –æ—Ç—Ç–æ–∫ "
                                    "–∫–ª–∏–µ–Ω—Ç–æ–≤?",
                    )

                    run_btn = gr.Button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", variant="primary")

                with gr.Column(scale=2):
                    status_output = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", interactive=False)
                    report_markdown = gr.Markdown(
                        label="üìë –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç", visible=False
                    )
                    images_gallery = gr.Gallery(
                        label="üìä –ì—Ä–∞—Ñ–∏–∫–∏",
                        columns=2,
                        height="auto",
                        object_fit="contain",
                        visible=False,
                    )

                    with gr.Row(visible=False) as download_row:
                        report_download = gr.File(label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç (.md)")
                        images_zip_download = gr.File(
                            label="üì• –°–∫–∞—á–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ (.zip)"
                        )

                    with gr.Group(visible=False) as qa_section:
                        gr.Markdown("### –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –æ—Ç—á–µ—Ç—É")

                        question_input = gr.Textbox(
                            label="–í–∞—à –≤–æ–ø—Ä–æ—Å",
                            placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å",
                        )
                        ask_btn = gr.Button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç")
                        answer_output = gr.Textbox(
                            label="–û—Ç–≤–µ—Ç", interactive=False, lines=5
                        )

        # --- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π ---
        fetch_models_btn.click(
            fetch_models,
            inputs=[api_key_input, base_url_input],
            outputs=[model_dropdown if OPENAI_AVAILABLE else model_input],
        )

        def save_and_show_status(api_key, base_url, model):
            status_message = save_api_settings(api_key, base_url, model)
            return status_message, gr.update(visible=True)

        def hide_status():
            return gr.update(visible=False)

        save_settings_btn.click(
            save_and_show_status,
            inputs=[
                api_key_input,
                base_url_input,
                model_dropdown if OPENAI_AVAILABLE else model_input,
            ],
            outputs=[settings_status, settings_status],
        ).then(
            hide_status,
            None,
            settings_status,
            queue=False,
        )

        def on_run_analysis(file_obj, api_key, base_url, model, question_for_target):
            (
                status,
                report_path,
                images,
                report_text,
                history,
            ) = run_analysis(
                file_obj, api_key, base_url, model, question_for_target
            )

            zip_path = create_zip_with_images(report_text)

            report_visible = bool(report_text)
            images_visible = bool(images)
            download_visible = bool(report_path or zip_path)
            qa_visible = bool(report_text)

            return (
                status,
                report_text if report_text else "–û—Ç—á–µ—Ç –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.",
                gr.update(visible=report_visible),
                images,
                gr.update(visible=images_visible),
                report_path if report_path and os.path.exists(
                    report_path) else None,
                zip_path if zip_path and os.path.exists(zip_path) else None,
                gr.update(visible=download_visible),
                gr.update(visible=qa_visible),
                report_text,
                history,
            )

        run_btn.click(
            on_run_analysis,
            inputs=[
                file_input,
                api_key_input,
                base_url_input,
                model_dropdown if OPENAI_AVAILABLE else model_input,
                question_for_target_input,
            ],
            outputs=[
                status_output,
                report_markdown,
                report_markdown,
                images_gallery,
                images_gallery,
                report_download,
                images_zip_download,
                download_row,
                qa_section,
                report_text_state,
                history_state,
            ],
        )

        def on_ask_question(question, report_text, api_key, base_url, model):
            answer = answer_question(question, report_text, api_key, base_url, model)
            return answer

        ask_btn.click(
            on_ask_question,
            inputs=[
                question_input,
                report_text_state,
                api_key_input,
                base_url_input,
                model_dropdown if OPENAI_AVAILABLE else model_input,
            ],
            outputs=[answer_output],
        )

    return demo


# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ---
if __name__ == "__main__":
    Path("tmp").mkdir(exist_ok=True)
    Path("logs").mkdir(exist_ok=True)
    Path("report/output/images").mkdir(parents=True, exist_ok=True)

    demo = build_interface()
    demo.launch(server_name="0.0.0.0", server_port=8502)