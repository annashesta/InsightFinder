# ui/gradio_app.py
"""
Gradio UI –¥–ª—è InsightFinder.
"""

import os
import io
import zipfile
import re
import base64
import tempfile
from pathlib import Path
from typing import Tuple, List, Optional

import gradio as gr
import pandas as pd

from core.pipeline import analyze_dataset
from core.logger import get_logger
from core.utils import find_binary_target

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print(
        "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'openai' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. "
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏: `pip install openai`"
    )

logger = get_logger(__name__, "gradio_app.log")


def call_llm_for_qa(report_text: str, question: str, api_key: str, base_url: str, model: str) -> str:
    if not OPENAI_AVAILABLE:
        return "‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'openai' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞."

    if not api_key or not base_url or not model:
        return "‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã API (–∫–ª—é—á, URL, –º–æ–¥–µ–ª—å)."

    try:
        client = OpenAI(api_key=api_key, base_url=base_url + "/v1")
        prompt = f"""
–û—Ç—á–µ—Ç:
{report_text}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ \
–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤—ã—à–µ –æ—Ç—á–µ—Ç–∞. 
–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —Ç–æ—á–Ω—ã–º. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –æ—Ç—á–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
"""
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=1024,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {str(e)}"


def call_llm_to_determine_target(
        question: str, columns: List[str], api_key: str, base_url: str, model: str
) -> str:
    if not OPENAI_AVAILABLE:
        return columns[0] if columns else ""

    if not api_key or not base_url or not model:
        return columns[0] if columns else ""

    try:
        client = OpenAI(api_key=api_key, base_url=base_url + "/v1")
        columns_str = "\n".join([f"- {col}" for col in columns])
        prompt = f"""
–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
–¢–µ–±–µ –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∞—è –∫–æ–ª–æ–Ω–∫–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π \
(—Ç–∞—Ä–≥–µ—Ç–æ–º) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤–æ–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:
{columns_str}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä–∞—è, –ø–æ —Ç–≤–æ–µ–º—É –º–Ω–µ–Ω–∏—é, 
—è–≤–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å. 
–ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω, –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é.

–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏:
"""
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=100,
        )
        target = response.choices[0].message.content.strip()
        if target in columns:
            return target
        else:
            return columns[0] if columns else ""
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞: {e}")
        return columns[0] if columns else ""


def run_analysis(
        file_obj,
        api_key: str,
        base_url: str,
        model: str,
        question_for_target: str,
) -> Tuple[str, str, str, str, str]:
    if not file_obj:
        return "‚ùå –§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.", "", "", "", ""

    if not question_for_target:
        return (
            "‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å.",
            "", "", "", ""
        )

    try:
        df = pd.read_csv(file_obj.name)
        logger.info("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª —á–µ—Ä–µ–∑ Gradio UI.")

        binary_cols = [col for col in df.columns if df[col].nunique() == 2]
        if binary_cols:
            determined_target = call_llm_to_determine_target(
                question_for_target, binary_cols, api_key, base_url, model
            )
            if determined_target in binary_cols:
                target_col = determined_target
                logger.info(f"üéØ –¢–∞—Ä–≥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω LLM: '{target_col}'")
            else:
                target_col = binary_cols[0]
                logger.info(
                    f"üéØ –¢–∞—Ä–≥–µ—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ø–µ—Ä–≤–∞—è –±–∏–Ω–∞—Ä–Ω–∞—è): '{target_col}'"
                )
        else:
            target_col = find_binary_target(df)
            logger.info(f"üéØ –ù–∞–π–¥–µ–Ω –±–∏–Ω–∞—Ä–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: '{target_col}'")

        unique_vals = df[target_col].dropna().unique()
        if len(unique_vals) != 2:
            return (
                f"‚ùå –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü '{target_col}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –±–∏–Ω–∞—Ä–Ω—ã–º. "
                f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {unique_vals}.",
                "", "", "", ""
            )

        with tempfile.NamedTemporaryFile(
                mode="w", delete=False, suffix=".csv", encoding="utf-8"
        ) as tmpfile:
            df.to_csv(tmpfile.name, index=False)
            tmp_path = tmpfile.name

        original_filename = os.path.basename(file_obj.name)
        report_path, history, report_text = analyze_dataset(tmp_path, target_col, original_filename)
        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω.")

        os.unlink(tmp_path)

        from report.to_html import markdown_to_html_with_images
        report_html = markdown_to_html_with_images(report_text)
        logger.info("‚úÖ –û—Ç—á–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ HTML.")

        return (
            "‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!",
            report_path,
            report_html,
            report_text,
            str(history),
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ run_analysis: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}", "", "", "", ""


def answer_question(
        question: str, report_text: str, api_key: str, base_url: str, model: str
) -> str:
    if not question:
        return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å."
    if not report_text:
        return "–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç—á–µ—Ç."

    logger.info(f"–í–æ–ø—Ä–æ—Å –ø–æ –æ—Ç—á–µ—Ç—É: {question}")
    answer = call_llm_for_qa(report_text, question, api_key, base_url, model)
    logger.info("‚úÖ –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—É—á–µ–Ω.")
    return answer


def create_zip_with_images(report_text: str) -> Optional[str]:
    if not report_text:
        return None

    try:
        pattern = r"!\[.*?\]\((.*?)\)"
        image_paths = re.findall(pattern, report_text)

        if not image_paths:
            return None

        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zip_file:
            base_images_dir = Path("report/output/images")
            added_files = set()

            for img_path_str in image_paths:
                clean_path_str = img_path_str
                if clean_path_str.startswith("images/"):
                    clean_path_str = clean_path_str[len("images/"):]
                elif clean_path_str.startswith("report/output/images/"):
                    clean_path_str = clean_path_str[len("report/output/images/"):]
                elif "report/output/images/" in clean_path_str:
                    parts = clean_path_str.split("report/output/images/")
                    if len(parts) > 1:
                        clean_path_str = parts[1]

                img_full_path = base_images_dir / clean_path_str
                if img_full_path.exists() and img_full_path not in added_files:
                    arcname = Path(clean_path_str).name
                    zip_file.write(img_full_path, arcname)
                    added_files.add(img_full_path)

        zip_buffer.seek(0)

        with tempfile.NamedTemporaryFile(
                delete=False, suffix=".zip"
        ) as tmp_zip:
            tmp_zip.write(zip_buffer.getvalue())
            return tmp_zip.name
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è ZIP —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {e}")
        return None


def fetch_models(api_key: str, base_url: str):
    if not api_key or not base_url:
        return gr.update(choices=[], value="", interactive=True)
    try:
        client = OpenAI(api_key=api_key, base_url=base_url + "/v1")
        models_response = client.models.list()
        model_ids = sorted([model.id for model in models_response.data])
        return gr.update(
            choices=model_ids,
            value=model_ids[0] if model_ids else "",
            interactive=True,
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return gr.update(choices=[], value="", interactive=True)


def save_api_settings(api_key: str, base_url: str, model: str) -> str:
    try:
        with open(".env", "w", encoding="utf-8") as f:
            f.write(f"OPENAI_API_KEY={api_key}\n")
            f.write(f"OPENAI_BASE_URL={base_url}\n")
            f.write(f"OPENAI_MODEL={model}\n")

        os.environ["OPENAI_API_KEY"] = api_key
        os.environ["OPENAI_BASE_URL"] = base_url
        os.environ["OPENAI_MODEL"] = model

        logger.info("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ .env")
        return "‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ API: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {str(e)}"


def save_html_report(html_content: str) -> str:
    if not html_content:
        return ""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".html", mode='w', encoding='utf-8') as f:
            f.write(html_content)
            return f.name
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML –æ—Ç—á–µ—Ç–∞: {e}")
        return ""


def build_interface():
    with gr.Blocks(title="InsightFinder", theme=gr.themes.Default()) as demo:
        gr.Markdown("# üß† InsightFinder ‚Äî AI –∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö")

        report_text_state = gr.State("")
        history_state = gr.State("")

        with gr.Tab("–ê–Ω–∞–ª–∏–∑"):
            with gr.Row():
                with gr.Column(scale=1):
                    file_input = gr.File(
                        label="üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª", file_types=[".csv"]
                    )

                    with gr.Accordion("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API", open=True):
                        api_key_input = gr.Textbox(
                            label="OPENAI_API_KEY",
                            type="password",
                            value=os.getenv("OPENAI_API_KEY", ""),
                        )
                        base_url_input = gr.Textbox(
                            label="OPENAI_BASE_URL",
                            value=os.getenv(
                                "OPENAI_BASE_URL",
                                "https://openai-hub.neuraldeep.tech"
                            ).strip(),
                        )

                        fetch_models_btn = gr.Button("üîÑ –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")

                        if OPENAI_AVAILABLE:
                            initial_models = []
                            initial_model = os.getenv(
                                "OPENAI_MODEL", "qwen2.5-32b-instruct"
                            )
                            try:
                                client = OpenAI(
                                    api_key=os.getenv("OPENAI_API_KEY", ""),
                                    base_url=os.getenv(
                                        "OPENAI_BASE_URL",
                                        "https://openai-hub.neuraldeep.tech"
                                    ) + "/v1",
                                )
                                models_response = client.models.list()
                                initial_models = sorted(
                                    [model.id for model in models_response.data]
                                )
                                if (initial_model not in initial_models and
                                        initial_models):
                                    initial_model = initial_models[0]
                            except Exception as e:
                                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏: {e}")

                            model_dropdown = gr.Dropdown(
                                choices=initial_models,
                                value=initial_model,
                                label="OPENAI_MODEL",
                                interactive=True,
                            )
                        else:
                            model_input = gr.Textbox(
                                label="OPENAI_MODEL",
                                value=os.getenv(
                                    "OPENAI_MODEL", "qwen2.5-32b-instruct"
                                ),
                            )

                        save_settings_btn = gr.Button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                        settings_status = gr.Textbox(
                            label="", interactive=False, visible=False
                        )

                    question_for_target_input = gr.Textbox(
                        label="‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ "
                              "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞—Ä–≥–µ—Ç–∞",
                        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –æ—Ç—Ç–æ–∫ "
                                    "–∫–ª–∏–µ–Ω—Ç–æ–≤?",
                    )

                    run_btn = gr.Button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", variant="primary")

                with gr.Column(scale=2):
                    status_output = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", interactive=False)
                    report_html_output = gr.HTML(
                        label="üìë –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç", visible=False
                    )

                    with gr.Row(visible=False) as download_row:
                        report_download = gr.File(label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç (.md)")
                        images_zip_download = gr.File(
                            label="üì• –°–∫–∞—á–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ (.zip)"
                        )
                        report_html_download = gr.File(label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç (.html)")

                    with gr.Group(visible=False) as qa_section:
                        gr.Markdown("### ‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –æ—Ç—á–µ—Ç—É")

                        question_input = gr.Textbox(
                            label="–í–∞—à –≤–æ–ø—Ä–æ—Å",
                            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫–æ–π –≥–ª–∞–≤–Ω—ã–π –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É—é—â–∏–π –ø—Ä–∏–∑–Ω–∞–∫?",
                        )
                        ask_btn = gr.Button("‚ùì –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç")
                        answer_output = gr.Textbox(
                            label="–û—Ç–≤–µ—Ç", interactive=False, lines=5
                        )

        fetch_models_btn.click(
            fetch_models,
            inputs=[api_key_input, base_url_input],
            outputs=[model_dropdown if OPENAI_AVAILABLE else model_input],
        )

        def save_and_show_status(api_key, base_url, model):
            status_message = save_api_settings(api_key, base_url, model)
            return status_message, gr.update(visible=True)

        def hide_status():
            return gr.update(visible=False)

        save_settings_btn.click(
            save_and_show_status,
            inputs=[
                api_key_input,
                base_url_input,
                model_dropdown if OPENAI_AVAILABLE else model_input,
            ],
            outputs=[settings_status, settings_status],
        ).then(
            hide_status,
            None,
            settings_status,
            queue=False,
        )

        def on_run_analysis(file_obj, api_key, base_url, model, question_for_target):
            original_filename = file_obj.name.split("/")[-1] if file_obj else "unknown.csv"

            status, report_path, report_html, report_text, history = run_analysis(
                file_obj, api_key, base_url, model, question_for_target
            )

            zip_path = create_zip_with_images(report_text)
            html_file_path = save_html_report(report_html)

            report_visible = bool(report_html)
            download_visible = bool(report_path or zip_path or html_file_path)
            qa_visible = bool(report_text)

            return (
                status,
                report_html if report_html else "<p>–û—Ç—á–µ—Ç –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.</p>",
                gr.update(visible=report_visible),
                report_path if report_path and os.path.exists(
                    report_path) else None,
                zip_path if zip_path and os.path.exists(zip_path) else None,
                html_file_path if html_file_path and os.path.exists(html_file_path) else None,
                gr.update(visible=download_visible),
                gr.update(visible=qa_visible),
                report_text,
                history,
            )

        run_btn.click(
            on_run_analysis,
            inputs=[
                file_input,
                api_key_input,
                base_url_input,
                model_dropdown if OPENAI_AVAILABLE else model_input,
                question_for_target_input,
            ],
            outputs=[
                status_output,
                report_html_output,
                report_html_output,
                report_download,
                images_zip_download,
                report_html_download,
                download_row,
                qa_section,
                report_text_state,
                history_state,
            ],
        )

        def on_ask_question(question, report_text, api_key, base_url, model):
            answer = answer_question(question, report_text, api_key, base_url, model)
            return answer

        ask_btn.click(
            on_ask_question,
            inputs=[
                question_input,
                report_text_state,
                api_key_input,
                base_url_input,
                model_dropdown if OPENAI_AVAILABLE else model_input,
            ],
            outputs=[answer_output],
        )

    return demo


if __name__ == "__main__":
    Path("tmp").mkdir(exist_ok=True)
    Path("logs").mkdir(exist_ok=True)
    Path("report/output/images").mkdir(parents=True, exist_ok=True)

    demo = build_interface()
    demo.launch(server_name="0.0.0.0", server_port=8502)
